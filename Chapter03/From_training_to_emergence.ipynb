{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Monday, March 11, \n",
        "\n",
        "This all runs in one pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwwAD03fS77X"
      },
      "source": [
        "# Working with GPT-4\n",
        "Copyright 2023, Denis Rothman, MIT License\n",
        "\n",
        "**Exploring emergence with [OpenAI ChatGPT Plus, GPT-4](https://openai.com/)**\n",
        "\n",
        "This notebook contains the excerpts of dialog with GPT-4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srKBvJZzTgUc"
      },
      "source": [
        "**Denis :** Write a k-nearest neighbor program in Python that uses 1000 random variables using a k-NN decision boundary, 5 classes and display the result with MatPlotlib:\n",
        "\n",
        "**GPT-4 explains(excerpt)**:<br>\n",
        "...\n",
        "The k-NN neighbors boundary is not a single, well-defined boundary but rather a collection of irregular and potentially disjoint boundaries, which can be complex and highly dependent on the specific dataset and the choice of 'k'. When 'k' is small, the decision boundaries can be quite sensitive to noise, leading to overfitting. On the other hand, when 'k' is too large, the decision boundaries may become too smooth, leading to underfitting. Choosing the optimal value of 'k' is crucial for achieving good performance with the k-NN algorithm.\n",
        ".../...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJXPIMeAS33o",
        "outputId": "481127a1-a16a-44d7-8690-1bb30f6fae67"
      },
      "outputs": [],
      "source": [
        "# !pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from matplotlib.colors import ListedColormap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate 1000 random data points with 5 classes\n",
        "X, y = make_blobs(n_samples=1000, centers=5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a k-NN classifier with k=5\n",
        "knn = KNeighborsClassifier(n_neighbors=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "KNeighborsClassifier()"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the classifier with the training data\n",
        "knn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a mesh to plot the decision boundaries\n",
        "h = .02  # step size in the mesh\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This puts a 100% load on a single CPU Core  ...\n",
        "# Predict the mesh points' class labels\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# 35.9s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#AFAFAF', '#FFD700'])\n",
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#808080', '#FFA500'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "cgHFQTRzLtnZ",
        "outputId": "38610c6b-1234-4b0a-b990-5d2d72932e80"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "# Plot the training points\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold, edgecolor='k', s=20)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.title(\"5-Class classification (k = 5, n_samples = 1000)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmy7XJPVN0ed"
      },
      "source": [
        "GPT-4 explains the code(excerpt:\n",
        "\n",
        "...\n",
        "This code creates a 5-class dataset using the make_blobs function and splits it into training and test sets. Then, it trains a k-NN classifier (with k=5) using the training data. Finally, it creates a mesh to visualize the decision boundaries and displays the resulting plot using Matplotlib.\n",
        "...<br>\n",
        "The provided code is for k-Nearest Neighbors (k-NN), not k-Means clustering. However, the visualization might have led to the confusion, as the data points appear clustered. Let me clarify the differences between k-NN and k-Means.\n",
        "\n",
        "k-Nearest Neighbors (k-NN) is a supervised learning algorithm used for classification and regression tasks. In k-NN, we are provided with labeled training data, and the goal is to predict the class (or value) of a new data point based on the 'k' closest training examples in the feature space. The new data point is assigned the majority class of its 'k' nearest neighbors.\n",
        "\n",
        "k-Means, on the other hand, is an unsupervised learning algorithm used for clustering tasks. It aims to partition the dataset into 'k' clusters based on similarity (usually Euclidean distance) between data points. The algorithm iteratively assigns each data point to the nearest centroid (cluster center) and updates the centroids based on the mean of the data points in each cluster.\n",
        "\n",
        "The main differences between k-NN and k-Means are:\n",
        "\n",
        "k-NN is a supervised learning algorithm, while k-Means is an unsupervised learning algorithm.\n",
        "k-NN is used for classification and regression tasks, whereas k-Means is used for clustering.\n",
        "k-NN uses labeled data for training, while k-Means does not require any labels for the data points.\n",
        "In k-NN, the \"k\" represents the number of nearest neighbors used for classification or regression. In k-Means, \"k\" represents the number of clusters.\n",
        "To clarify, the provided code demonstrates the k-Nearest Neighbors algorithm for classification with 5 classes. The use of make_blobs function generates a synthetic dataset that visually appears clustered, but the algorithm applied is indeed k-NN.<br>\n",
        "...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
