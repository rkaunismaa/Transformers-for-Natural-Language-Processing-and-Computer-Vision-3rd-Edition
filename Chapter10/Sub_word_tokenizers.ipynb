{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tuesday, April 2, 2024\n",
        "\n",
        "mamba install \n",
        "\n",
        "This all runs in one pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uzUPYCWLnjc"
      },
      "source": [
        "#Sub_word_tokenizers\n",
        "Copyright 2023, Denis Rothman\n",
        "\n",
        "Sub word tokenizers other than BPE and Wordpiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BfR0YFi1Lj5l"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Whr2POZZSV85"
      },
      "outputs": [],
      "source": [
        "# !pip install sentencepiece -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYfm1hecNBC5"
      },
      "source": [
        "# Unigram Language Model Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzvI78jzN5BT",
        "outputId": "a28482da-22ae-4909-fdff-ee4b9122eb34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "['S', 'ubword', 'tokeniz', 'er', 's', 'break', 'te', 'x', 't', 'se', 'q', 'u', 'ence', 's', 'in', 'to', 'subword', 's', '.']\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import Unigram\n",
        "from tokenizers.trainers import UnigramTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Define a sample corpus\n",
        "corpus = [\n",
        "    \"Subword tokenizers break text sequences into subwords.\",\n",
        "    \"This sentence is another part of the corpus.\",\n",
        "    \"Tokenization is the process of breaking text down into smaller units.\",\n",
        "    \"These smaller units can be words, subwords, or even individual characters.\",\n",
        "    \"Transformer models often use subword tokenization.\"\n",
        "]\n",
        "\n",
        "# Instantiate a Unigram tokenizer model\n",
        "tokenizer = Tokenizer(Unigram([]))\n",
        "\n",
        "# Add a pre-tokenizer\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train the tokenizer model\n",
        "trainer = UnigramTrainer(vocab_size=5000)  # Here you set the desired vocabulary size\n",
        "tokenizer.train_from_iterator(corpus, trainer)\n",
        "\n",
        "# Now let's tokenize the original sentence\n",
        "output = tokenizer.encode(\"Subword tokenizers break text sequences into subwords.\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9fLr_KeTOIr"
      },
      "source": [
        "# SentencePiece tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__cUMfsNTtgu",
        "outputId": "a71f6776-199c-41a8-dcee-65ae5b9ff678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁', 'S', 'ubword', '▁tokeniz', 'ers', '▁break', '▁', 'te', 'x', 't', '▁se', 'q', 'u', 'ence', 's', '▁in', 'to', '▁subwords', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: large_corpus.txt\n",
            "  input_format: \n",
            "  model_prefix: m\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 88\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: large_corpus.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 10000 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=591674\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=29\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 10000 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=524806\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 110 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 10000\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 37\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 37 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=89 obj=15.1242 num_tokens=123 num_tokens/piece=1.38202\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=83 obj=12.6945 num_tokens=123 num_tokens/piece=1.48193\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "import random\n",
        "\n",
        "# Define a basic corpus\n",
        "basic_corpus = [\n",
        "    \"Subword tokenizers break text sequences into subwords.\",\n",
        "    \"This sentence is another part of the corpus.\",\n",
        "    \"Tokenization is the process of breaking text down into smaller units.\",\n",
        "    \"These smaller units can be words, subwords, or even individual characters.\",\n",
        "    \"Transformer models often use subword tokenization.\"\n",
        "]\n",
        "\n",
        "# Generate a larger corpus by repeating sentences from the basic corpus\n",
        "corpus = [random.choice(basic_corpus) for _ in range(10000)]\n",
        "\n",
        "# Write the corpus to a text file\n",
        "with open('large_corpus.txt', 'w') as f:\n",
        "    for sentence in corpus:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "# Train the SentencePiece model\n",
        "spm.SentencePieceTrainer.train(input='large_corpus.txt', model_prefix='m', vocab_size=88)\n",
        "\n",
        "# Load the trained model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# Tokenize the original sentence\n",
        "tokens = sp.encode_as_pieces(\"Subword tokenizers break text sequences into subwords.\")\n",
        "print(tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPx9F7HkzPZJBxN2EfKh0ss",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
