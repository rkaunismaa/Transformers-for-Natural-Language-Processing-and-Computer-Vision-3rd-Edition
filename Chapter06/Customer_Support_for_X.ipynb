{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgc4A8eoman6"
      },
      "source": [
        "# Pretraining a Customer Support Model on X (former Twitter) Data\n",
        "copyright 2023-2024, Denis Rothman\n",
        "\n",
        "This is an educational notebook to show how to implement a Hugging Face RoRobertaForCausalLM model on messages on X(former Twitter). The goal is only to show the method(see limitations below).\n",
        "\n",
        "**Pretraining a Generative AI model from scratch**\n",
        "\n",
        "**Dataset:**Tweets from 20 Top Brands by Volume  \n",
        "**Model:**  RobertaForCausalLM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UybE1zASmTqE"
      },
      "source": [
        "![](https://i.imgur.com/nTv3Iuu.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1oqh0F6W3ad"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "The goal of the notebook is to train a Hugging Face RobertaForCausalLM model to simulate a customer support chat agent for X (former Twitter)\n",
        "\n",
        "This notebook requires a GPU.\n",
        "\n",
        "**Customer Support on Twitter**\n",
        "Over 2 million tweets and replies from the biggest brands on Twitter\n",
        "\n",
        "https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter\n",
        "\n",
        "**Limitations**:\n",
        "\n",
        "The scope of pretraining was limited to a subset of the dataset for time constraints. You can train the full dataset on Google Colab or another platform. You can also select another model if you find the generalized reponses insufficient.The reponses are only there to show how the system workds.\n",
        "\n",
        "RoBERTa is not a standard generative AI model such as GPT models as in the Chapter07 directory. However, it can be implemented as a reasonably interesting autoregressive(token by token loop) model that illustrates how to begin to explore how generative AI works. \n",
        "\n",
        "In the following chapters we will be using **GPT-4** and other **LLM** models. *However, exploring smaller open source models for a specific domain can sometimes provide everything we need for our project.* \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-oZZR__a_sB",
        "outputId": "eb47d282-4cab-4d47-874d-fded579cb309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFEtWPN8h9uX"
      },
      "source": [
        "Kaggle credentials for authentification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNU0N4GSiusu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "with open(os.path.expanduser(\"drive/MyDrive/files/kaggle.json\"), \"r\") as f:\n",
        "    kaggle_credentials = json.load(f)\n",
        "\n",
        "kaggle_username = kaggle_credentials[\"username\"]\n",
        "kaggle_key = kaggle_credentials[\"key\"]\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = kaggle_username\n",
        "os.environ[\"KAGGLE_KEY\"] = kaggle_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpU3TjuwjBfR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import kaggle\n",
        "except:\n",
        "  !pip install kaggle\n",
        "  import kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs7YkSkajEzZ"
      },
      "outputs": [],
      "source": [
        "kaggle.api.authenticate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8T7j_wfP7g0"
      },
      "source": [
        "#Step 1: Downloading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrugMLZodf3I"
      },
      "source": [
        "https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvLLPezndky7",
        "outputId": "a05c52f0-1db7-4ecd-b42c-0a81471457e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading customer-support-on-twitter.zip to /content\n",
            " 97% 164M/169M [00:06<00:00, 32.9MB/s]\n",
            "100% 169M/169M [00:06<00:00, 27.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d thoughtvector/customer-support-on-twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVIuNoupezb4",
        "outputId": "c602ed95-5572-4717-dc8d-3399f4aee58e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Unzipped!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/customer-support-on-twitter.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "\n",
        "print(\"File Unzipped!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC7YmlYQQIAy"
      },
      "source": [
        "#Step 2: Installing Hugging Face transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc-btAX_3Flw"
      },
      "source": [
        "April 2023 update From Hugging Face Issue 22816:\n",
        "\n",
        "https://github.com/huggingface/transformers/issues/22816\n",
        "\n",
        "\"The PartialState import was added as a dependency on the transformers development branch yesterday. PartialState was added in the 0.17.0 release in accelerate, and so for the development branch of transformers, accelerate >= 0.17.0 is required.\n",
        "\n",
        "Downgrading the transformers version removes the code which is importing PartialState.\"\n",
        "\n",
        "Denis Rothman: The following cell imports the latest version of Hugging Face transformers but without downgrading it.\n",
        "\n",
        "To adapt to the Hugging Face upgrade, A GPU accelerator was activated using the Google Colab Pro with the following NVIDIA GPU:\n",
        "GPU Name: NVIDIA A100-SXM4-40GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vr8JwVR3Flx",
        "outputId": "115436d9-c327-444d-cef3-5a45e0314ace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from Transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from Transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from Transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from Transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from Transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from Transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from Transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from Transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from Transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->Transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->Transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, Transformers\n",
            "Successfully installed Transformers-4.32.1 huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Transformers\n",
        "!pip install --upgrade accelerate\n",
        "from accelerate import Accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPxgiBeFjS87"
      },
      "source": [
        "creating subdirectories to store the datasets, the logs and the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K41xswZZAQYM"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/model/dataset/\n",
        "!mkdir -p /content/model/model/\n",
        "!mkdir -p /content/model/logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YNuf3NQ-z1J"
      },
      "source": [
        "# Step 3:  Loading and filtering the data\n",
        "\n",
        "We will use a subset of the dataset to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZXD5Rfmg1EZ",
        "outputId": "dfc9b3b0-5997-49a4-8f4d-ff2349556652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   tweet_id   author_id  inbound                      created_at  \\\n",
            "0         1  sprintcare    False  Tue Oct 31 22:10:47 +0000 2017   \n",
            "1         2      115712     True  Tue Oct 31 22:11:45 +0000 2017   \n",
            "2         3      115712     True  Tue Oct 31 22:08:27 +0000 2017   \n",
            "3         4  sprintcare    False  Tue Oct 31 21:54:49 +0000 2017   \n",
            "4         5      115712     True  Tue Oct 31 21:49:35 +0000 2017   \n",
            "\n",
            "                                                text response_tweet_id  \\\n",
            "0  @115712 I understand. I would like to assist y...                 2   \n",
            "1      @sprintcare and how do you propose we do that               NaN   \n",
            "2  @sprintcare I have sent several private messag...                 1   \n",
            "3  @115712 Please send us a Private Message so th...                 3   \n",
            "4                                 @sprintcare I did.                 4   \n",
            "\n",
            "   in_response_to_tweet_id  \n",
            "0                      3.0  \n",
            "1                      1.0  \n",
            "2                      4.0  \n",
            "3                      5.0  \n",
            "4                      6.0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/twcs/twcs.csv')\n",
        "\n",
        "# Check the first few rows to understand the data\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwkWCioCg9Pr"
      },
      "source": [
        "Extracting relevant data\n",
        "\n",
        "In this case, we are extracting the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG2aemoxhA2b"
      },
      "outputs": [],
      "source": [
        "# Extract tweets from the 'text' column or any other relevant column\n",
        "tweets = df['text'].dropna().tolist()  # This assumes the column with tweets is named 'text'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i56Txp5seS0R"
      },
      "outputs": [],
      "source": [
        "# Convert the list of tweets to a DataFrame\n",
        "df_tweets = pd.DataFrame(tweets, columns=['text'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_tweets.to_csv('tweets.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmd67MS3erjy",
        "outputId": "c6d48299-9ebb-4c64-d314-48471b94c518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2,811,774\n"
          ]
        }
      ],
      "source": [
        "# Checking the length of df\n",
        "formatted_length = \"{:,}\".format(len(df_tweets))\n",
        "print(formatted_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2B-RjrK_fQW"
      },
      "source": [
        "Checking the extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16r_Aj8m_Ke-",
        "outputId": "688048b3-c7f8-4e20-b9ce-42f8c6b9c89a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@115712 I understand. I would like to assist you. We would need to get you into a private secured link to further assist.\n",
            "@sprintcare and how do you propose we do that\n",
            "@sprintcare I have sent several private messages and no one is responding as usual\n",
            "@115712 Please send us a Private Message so that we can further assist you. Just click ‘Message’ at the top of your profile.\n",
            "@sprintcare I did.\n",
            "@115712 Can you please send us a private message, so that I can gain further details about your account?\n",
            "@sprintcare is the worst customer service\n",
            "@115713 This is saddening to hear. Please shoot us a DM, so that we can look into this for you. -KC\n",
            "@sprintcare You gonna magically change your connectivity for me and my whole family ? 🤥 💯\n",
            "@115713 We understand your concerns and we'd like for you to please send us a Direct Message, so that we can further assist you. -AA\n"
          ]
        }
      ],
      "source": [
        "for tweet in tweets[:10]:  # This will display the first 5 tweets\n",
        "    print(tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hql9rvjkDBQU"
      },
      "source": [
        "filtering the extraction to clean it and apply lowercase conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du57v2vxC_6k"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def filter_tweet(tweet):\n",
        "    # Keep only characters a to z, spaces, and apostrophes, then convert to lowercase\n",
        "    return re.sub(r'[^a-z\\s\\']', '', tweet.lower())\n",
        "\n",
        "filtered_tweets = [filter_tweet(tweet) for tweet in tweets]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKO1BK7ADQRD"
      },
      "outputs": [],
      "source": [
        "f=30\n",
        "filtered_tweets = [tweet for tweet in filtered_tweets if len(tweet.split()) > f]  # Only keep tweets with more than f words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KR-YnlNDoOj",
        "outputId": "ff7b5a98-31a3-4364-8c8c-f51669c83c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "marksandspencer i check with the gov office and legal they stated you are not right but its funny how the other stores dont but you do no wonder lidl and the rest are beating you\n",
            "marksandspencer ou must charge at least p a bag including vat for carrier bags that are all of the following\n",
            "\n",
            "unused  its new and hasnt already been used for sold goods to be taken away or delivered\n",
            "plastic and  microns thick or less\n",
            "it has handles an opening and isnt sealed\n",
            "marksandspencer arent require charge  a bag\n",
            "paper bags\n",
            "shops in airports or on board trains aeroplanes or ships\n",
            "bags which only contain certain items such as unwrapped food raw meat and fish where there is a food safety risk prescription medicines uncovered blades seeds bulbs amp s\n",
            " hi you can change your microsoft account email through the steps here httpstcodkehohboyy  if the email your son wants to change to is already associated with a microsoft account you'll need to follow those steps to switch the email address on that account too zm\n",
            "xboxsupport this is what's on my screen not sure if it's xbox side or my side that's broken last time it was my as had to call head office about it httpstcojnmivolnbs\n",
            "airasiasupport hi i would like to know on my itenerary on how the birthday can be seen i would like to double check if i can key in our birthdays right \n",
            "morrisons that's pointless as it's every day all the time no matter when i go in i speak the management who r as fed up as i am they've no staff\n",
            "delta ok it was not a full flight equipment change had nothing to do with comfort class no disabilities and a handling agent telling customer care would take care of us so please non of these as an excuse ill retwitter your reply thanks\n",
            "delta obviously you didn't read what already was said before you do give exactly the same bs answers i know there is no guarantee but i like to know why they moved us i know because a delta employee was sitting in my seat and you can't say that \n",
            "hi britishairways my flight from manlhrbwi for nov  was canceled i was excited to try your club  product only available flight is now to iad which is a hassle but rebooked anywaymy only option any availability in first class on ba for the troubles please\n"
          ]
        }
      ],
      "source": [
        "for filtered_tweet in filtered_tweets[:10]:  # This will display the first 5 tweets\n",
        "    print(filtered_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-1tjZm-Dedv",
        "outputId": "eb12b961-1163-4ebb-cc2c-35ca9c45ad38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "228,637\n"
          ]
        }
      ],
      "source": [
        "# Checking the length of dataset\n",
        "formatted_length = \"{:,}\".format(len(filtered_tweets))\n",
        "print(formatted_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_PQGgwTFWsy"
      },
      "source": [
        "save the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvoe_tsPEqZa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "# Save to CSV\n",
        "with open('/content/model/dataset/processed_tweets.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for tweet in filtered_tweets:\n",
        "        writer.writerow([tweet])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe5km5L5FUjx"
      },
      "source": [
        "check the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMIMjyPMFYL7",
        "outputId": "db9a21ca-bd94-4c0d-c92f-8b5b96e2b787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "marksandspencer i check with the gov office and legal they stated you are not right but its funny how the other stores dont but you do no wonder lidl and the rest are beating you\n",
            "marksandspencer ou must charge at least p a bag including vat for carrier bags that are all of the following\n",
            "\n",
            "unused  its new and hasnt already been used for sold goods to be taken away or delivered\n",
            "plastic and  microns thick or less\n",
            "it has handles an opening and isnt sealed\n",
            "marksandspencer arent require charge  a bag\n",
            "paper bags\n",
            "shops in airports or on board trains aeroplanes or ships\n",
            "bags which only contain certain items such as unwrapped food raw meat and fish where there is a food safety risk prescription medicines uncovered blades seeds bulbs amp s\n",
            " hi you can change your microsoft account email through the steps here httpstcodkehohboyy  if the email your son wants to change to is already associated with a microsoft account you'll need to follow those steps to switch the email address on that account too zm\n",
            "xboxsupport this is what's on my screen not sure if it's xbox side or my side that's broken last time it was my as had to call head office about it httpstcojnmivolnbs\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# Read from CSV\n",
        "with open('/content/model/dataset/processed_tweets.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "\n",
        "    # Use islice from itertools to only get the first 5 lines\n",
        "    from itertools import islice\n",
        "    for row in islice(reader, 5):\n",
        "        print(row[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NlHvIhmRUSO"
      },
      "source": [
        "#Step 4: Checking Resource Constraints: GPU and CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD140sFjh0LQ",
        "outputId": "f853ae38-75f0-4e0d-dcec-efb896803034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Sep  3 10:28:07 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    25W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNZZs-r6iKAV"
      },
      "outputs": [],
      "source": [
        "#@title Checking that PyTorch Sees CUDA\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfX9F1iRgiR"
      },
      "source": [
        "#Step 5: Defining the configuration of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTXXutqeDzPi"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaConfig, RobertaForCausalLM\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        "    is_decoder=True,  # Set up the model for potential seq2seq use, allowing for autoregressive outputs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-UsuK9Ps0H7"
      },
      "outputs": [],
      "source": [
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-HTOcL0HKx1"
      },
      "source": [
        "define and print model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ4AQPYmHMMD"
      },
      "outputs": [],
      "source": [
        "# Create the RobertaForCausalLM model with the specified config\n",
        "model = RobertaForCausalLM(config=config)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Illu-7KnRlbC"
      },
      "source": [
        "##  defining the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4keFBUjQFOD1"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# Initialize the tokenizer using the 'roberta-base' pre-trained model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxqMcoe-nkTl"
      },
      "outputs": [],
      "source": [
        "# Display special tokens\n",
        "print(\"Special tokens:\", tokenizer.special_tokens_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liD4DyEiRtVq"
      },
      "source": [
        "## Exploring the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU6JhBSTKiaM"
      },
      "outputs": [],
      "source": [
        "print(model.num_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BXhhe7twTxb"
      },
      "outputs": [],
      "source": [
        "LP=list(model.parameters())\n",
        "lp=len(LP)\n",
        "print(lp)\n",
        "for p in range(0,lp):\n",
        "  print(LP[p])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7Tb1HulWsH2"
      },
      "outputs": [],
      "source": [
        "#Shape of each tensor in the model\n",
        "LP = list(model.parameters())\n",
        "for i, tensor in enumerate(LP):\n",
        "    print(f\"Shape of tensor {i}: {tensor.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej82kG6K3akQ"
      },
      "outputs": [],
      "source": [
        "#counting the parameters\n",
        "np=0\n",
        "for p in range(0,lp):#number of tensors\n",
        "  PL2=True\n",
        "  try:\n",
        "    L2=len(LP[p][0]) #check if 2D\n",
        "  except:\n",
        "    L2=1             #not 2D but 1D\n",
        "    PL2=False\n",
        "  L1=len(LP[p])\n",
        "  L3=L1*L2\n",
        "  np+=L3             # number of parameters per tensor\n",
        "  if PL2==True:\n",
        "    print(p,L1,L2,L3)  # displaying the sizes of the parameters\n",
        "  if PL2==False:\n",
        "    print(p,L1,L3)  # displaying the sizes of the parameters\n",
        "\n",
        "print(np)              # total number of parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD2ZWZNESMze"
      },
      "source": [
        "# Step 6: Creating and processing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3TNl3JDKDsQ"
      },
      "outputs": [],
      "source": [
        "#installing Hugging Face datasets for data loading and preprocessing\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-O07CaVKPA-"
      },
      "outputs": [],
      "source": [
        "#load dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('csv', data_files='/content/model/dataset/processed_tweets.csv', column_names=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccLjzIKcKVyZ"
      },
      "outputs": [],
      "source": [
        "# split datasets into train and eval\n",
        "from datasets import DatasetDict\n",
        "\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)  # 10% for evaluation\n",
        "dataset = DatasetDict(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE4-dPhbKcwz"
      },
      "outputs": [],
      "source": [
        "# Tokenize datasets:\n",
        "# - If a record's length is less than `max_length`, it's padded to ensure all records have the same length.\n",
        "# - If a record's length exceeds `max_length`, it's truncated to the specified max length.\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzMXOx4_LOxl"
      },
      "outputs": [],
      "source": [
        "# datacollator to batch items together for training and evaluation\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Define the data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # For causal (autoregressive) language modeling\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zCqbnY_STtz"
      },
      "source": [
        "# Step 7: Initializing the trainer\n",
        "\n",
        "The number of epochs can be empirically increased until the\n",
        "accuracy versus training time reaches a limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qii0E50ItZjq"
      },
      "outputs": [],
      "source": [
        "# to display the time every x steps suring training\n",
        "from transformers import Trainer\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def log(self, logs: Dict[str, Any]) -> None:\n",
        "        super().log(logs)\n",
        "        if \"step\" in logs:  # Check if \"step\" key is in the logs dictionary\n",
        "            step = int(logs[\"step\"])\n",
        "            if step % self.args.eval_steps == 0:\n",
        "                print(f\"Current time at step {step}: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tavXhZujTDGa"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Set up Python logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/model/model/\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,                  # can be increased to increase accuracy if productive\n",
        "    per_device_train_batch_size=64,      # batch size per device\n",
        "    save_steps=10_000,                   # save a checkpoint every save_steps=10000\n",
        "    save_total_limit=2,                  # the maximum number of checkpoint model files to keep\n",
        "    logging_dir='/content/model/logs/',  # directory for storing logs\n",
        "    logging_steps=100,                   # Log every 100 steps\n",
        "    logging_first_step=True,             # Log the first step\n",
        "    evaluation_strategy=\"steps\",         # Evaluate every \"eval_steps\"\n",
        "    eval_steps=500,                      # Evaluate every 500 steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpvnFFmZJD-N"
      },
      "outputs": [],
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset = tokenized_datasets[\"train\"],\n",
        "    eval_dataset = tokenized_datasets[\"test\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYYDdq2LSXAv"
      },
      "source": [
        "# Step 8: Pretraining the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmaHZXzmkNtJ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPdTe8KPUlkT"
      },
      "source": [
        "**Sample run information:**\n",
        "\n",
        "CPU times: user 26min 37s, sys: 3.01 s, total: 26min 40s\n",
        "Wall time: 26min 35s\n",
        "\n",
        "\n",
        "TrainOutput(global_step=3216, training_loss=5.05824806411468, metrics={'train_runtime': 1595.3194, \n",
        "\n",
        "'train_samples_per_second': 128.985, 'train_steps_per_second': 2.016, 'total_flos': 6822770940370944.0, \n",
        "\n",
        "'train_loss': 5.05824806411468, 'epoch': 1.0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUDyC0fFOIHb"
      },
      "source": [
        "display results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hqo1jy_tKhNY"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ40Ji_IOK6c"
      },
      "source": [
        "evaluate the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFPKg-kZOSiq"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-2hD_OLSa81"
      },
      "source": [
        "#Step 9: Saving the trained model (+tokenizer + config) to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDNgPls7_l13"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"/content/model/model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inHEZyaG0BRM"
      },
      "outputs": [],
      "source": [
        "#Uncomment the following line to save the output for future use\n",
        "#trainer.save_model(\"drive/MyDrive/files/model_C6/model/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD5ysD1sTdPh"
      },
      "source": [
        "# Step 10: User Interface to Chat with the Generative AI Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-tbraclWEqu"
      },
      "outputs": [],
      "source": [
        "# For standalone run : transformer library\n",
        "#!pip install Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJo8gQzHWJzH",
        "outputId": "942b8d42-894d-4a4f-c5bd-64fb51763bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#1.A.for standalone run : mount Google Drive and path to pretrained model\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model_path=\"drive/MyDrive/files/model_C6/model/\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xZ-CaJiVV1j3"
      },
      "outputs": [],
      "source": [
        "# 1.A For a run during the training session of this notebook,\n",
        "# Load the trained model: model path\n",
        "# local model path(comment for a standalone run):\n",
        "model_path=\"/content/model/model/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O9tU-RS3W9rf"
      },
      "outputs": [],
      "source": [
        "# 1.B Load the trained model and tokenizer : model and tokenizer\n",
        "from transformers import RobertaConfig, RobertaForCausalLM\n",
        "from transformers import RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForCausalLM.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPr6wtPLXVMB"
      },
      "source": [
        "## Running samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me-ujeTQSMhk",
        "outputId": "5a37eff8-c7ea-40ca-b06f-0c09f75b54d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I would like to know why they moved us and then they are not able to get the iphone x and then they are not able to help you out with the iphone x\n"
          ]
        }
      ],
      "source": [
        "# 2. Tokenize an input prompt\n",
        "prompt = \"I would like to know why they moved us\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=50, truncation=True)\n",
        "\n",
        "# 3. Generate a response from the model\n",
        "output = model.generate(**inputs, max_length=100, temperature=0.9, num_return_sequences=1)\n",
        "\n",
        "# 4. Decode the generated output\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3PTUsfoZuBE"
      },
      "source": [
        "Example:    \n",
        "Input   \n",
        "I would like to assist       \n",
        "output:   \n",
        "I would like to assist you please give us your full name address and phone number so we can look into this for you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lRJ9jlUZjWN"
      },
      "source": [
        "## Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "zy-N5rdDceRB",
        "outputId": "f6e96935-8566-4e6f-be6f-5dc89f87c105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, width, height, cache, element) => {\n                        if (!google.colab.kernel.accessAllowed && !cache) {\n                            return;\n                        }\n                        element.appendChild(document.createTextNode(''));\n                        const url = await google.colab.kernel.proxyPort(port, {cache});\n\n                        const external_link = document.createElement('div');\n                        external_link.innerHTML = `\n                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n                                    https://localhost:${port}${path}\n                                </a>\n                            </div>\n                        `;\n                        element.appendChild(external_link);\n\n                        const iframe = document.createElement('iframe');\n                        iframe.src = new URL(path, url).toString();\n                        iframe.height = height;\n                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n                        iframe.width = width;\n                        iframe.style.border = 0;\n                        element.appendChild(iframe);\n                    })(7860, \"/\", \"100%\", 500, false, window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from transformers import RobertaTokenizer, RobertaForCausalLM\n",
        "\n",
        "# Define the function to generate response\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=50, truncation=True)\n",
        "    output = model.generate(**inputs, max_length=200, temperature=0.9, num_return_sequences=1)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Create widgets\n",
        "text_input = widgets.Textarea(\n",
        "    description='Prompt:',\n",
        "    placeholder='Enter your prompt here...'\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description='Generate',\n",
        "    button_style='success'\n",
        ")\n",
        "\n",
        "output_text = widgets.Output(layout={'border': '1px solid black', 'height': '100px'})\n",
        "\n",
        "# Define button click event handler\n",
        "def on_button_clicked(b):\n",
        "    with output_text:\n",
        "        clear_output()\n",
        "        response = generate_response(text_input.value)\n",
        "        print(response)\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "# Display widgets\n",
        "display(text_input, button, output_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
