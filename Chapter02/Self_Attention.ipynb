{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wednesday, March 20, 2024\n",
    "\n",
    "This will be my example of the lessons learned in Chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Represent the Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This represents the token embeddings for every token in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input : 3 inputs, d_model=4\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Input : 3 inputs, d_model=4\")\n",
    "x =np.array([[1.0, 0.0, 1.0, 0.0],   # Input 1\n",
    "             [0.0, 2.0, 0.0, 2.0],   # Input 2\n",
    "             [1.0, 1.0, 1.0, 1.0]])  # Input 3\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Intializing the weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random initialized weight matrics, where the dimensioins are driven by the number of inputs and the embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: weights 3 dimensions x d_model=4\n",
      "w_query\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: weights 3 dimensions x d_model=4\")\n",
    "print(\"w_query\")\n",
    "w_query =np.array([[1, 0, 1],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [0, 1, 1]])\n",
    "print(w_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_key\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"w_key\")\n",
    "w_key =np.array([[0, 0, 1],\n",
    "                 [1, 1, 0],\n",
    "                 [0, 1, 0],\n",
    "                 [1, 1, 0]])\n",
    "print(w_key.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_value\n",
      "(4, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"w_value\")\n",
    "w_value = np.array([[0, 2, 0],\n",
    "                    [0, 3, 0],\n",
    "                    [1, 0, 3],\n",
    "                    [1, 1, 0]])\n",
    "print(w_value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Matrix multiplication to obtain Q, K, and V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we multiply a (3x4) matrix (tokens x embeddings )by a (4x3) matrix of random numbers to produce a (3x3) matrix, a (token x token) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Matrix multiplication to obtain Q,K,V\n",
      "Queries: x * w_query\n",
      "[[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
    "\n",
    "print(\"Queries: x * w_query\")\n",
    "Q=np.matmul(x,w_query)\n",
    "print(Q)\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Matrix multiplication to obtain Q,K,V\n",
      "Keys: x * w_key\n",
      "[[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
    "\n",
    "print(\"Keys: x * w_key\")\n",
    "K=np.matmul(x,w_key)\n",
    "print(K)\n",
    "print(K.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: x * w_value\n",
      "[[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Values: x * w_value\")\n",
    "V=np.matmul(x,w_value)\n",
    "print(V)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Scaled attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Scaled Attention Scores\n",
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Scaled Attention Scores\")\n",
    "k_d=1   #square root of k_d simplified to 1 for this example\n",
    "attention_scores = (Q @ K.transpose())/k_d\n",
    "print(attention_scores)\n",
    "print(attention_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Softmax side note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Compute the softmax function.\n",
    "\n",
    "The softmax function transforms each element of a collection by computing the exponential of each element divided by the sum of the exponentials of all the elements. That is, if x is a one-dimensional numpy array:\n",
    "\n",
    "    softmax(x) = np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)\n",
    "someArray = np.array([[1, 0.5, 0.2, 3],\n",
    "                      [1,  -1,   7, 3],\n",
    "                      [2,  12,  13, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the softmax transformation over the entire array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.48308990e-06, 2.71913148e-06, 2.01438214e-06, 3.31258028e-05],\n",
       "       [4.48308990e-06, 6.06720242e-07, 1.80860755e-03, 3.31258028e-05],\n",
       "       [1.21863018e-05, 2.68421160e-01, 7.29644362e-01, 3.31258028e-05]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = softmax(someArray)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the softmax transformation along the first axis (i.e., the columns).\n",
    "\n",
    ">>> m = softmax(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.11941558e-01, 1.01299681e-05, 2.75393864e-06, 3.33333333e-01],\n",
       "       [2.11941558e-01, 2.26030140e-06, 2.47261635e-03, 3.33333333e-01],\n",
       "       [5.76116885e-01, 9.99987610e-01, 9.97524630e-01, 3.33333333e-01]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m =  softmax(someArray, axis=0)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the softmax transformation along the second axis (i.e., the rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.05877070e-01, 6.42176889e-02, 4.75736340e-02, 7.82331607e-01],\n",
       "       [2.42746030e-03, 3.28521027e-04, 9.79307378e-01, 1.79366403e-02],\n",
       "       [1.22093673e-05, 2.68929212e-01, 7.31025390e-01, 3.31885014e-05]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = softmax(someArray, axis=1)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Scaled softmax attention scores for each vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here we replace the attention_scores above with this ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Scaled softmax attention_scores for each vector\n",
      "[0.06337894 0.46831053 0.46831053]\n",
      "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
      "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Scaled softmax attention_scores for each vector\")\n",
    "attention_scores[0]=softmax(attention_scores[0])\n",
    "attention_scores[1]=softmax(attention_scores[1])\n",
    "attention_scores[2]=softmax(attention_scores[2])\n",
    "print(attention_scores[0])\n",
    "print(attention_scores[1])\n",
    "print(attention_scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(attention_scores[0].sum())\n",
    "print(attention_scores[1].sum())\n",
    "print(attention_scores[2].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: The final attention representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: attention value obtained by score1/k_d * V\n",
      "[1. 2. 3.]\n",
      "[2. 8. 0.]\n",
      "[2. 6. 3.]\n",
      "Attention 1\n",
      "[0.06337894 0.12675788 0.19013681]\n",
      "Attention 2\n",
      "[0.93662106 3.74648425 0.        ]\n",
      "Attention 3\n",
      "[0.93662106 2.80986319 1.40493159]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6: attention value obtained by score1/k_d * V\")\n",
    "print(V[0])\n",
    "print(V[1])\n",
    "print(V[2])\n",
    "print(\"Attention 1\")\n",
    "attention1=attention_scores[0].reshape(-1,1)\n",
    "attention1=attention_scores[0][0]*V[0]\n",
    "print(attention1)\n",
    "\n",
    "print(\"Attention 2\")\n",
    "attention2=attention_scores[0][1]*V[1]\n",
    "print(attention2)\n",
    "\n",
    "print(\"Attention 3\")\n",
    "attention3=attention_scores[0][2]*V[2]\n",
    "print(attention3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Summing up the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: summed the results to create the first line of the output matrix\n",
      "[1.93662106 6.68310531 1.59506841]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 7: summed the results to create the first line of the output matrix\")\n",
    "attention_input1=attention1+attention2+attention3\n",
    "print(attention_input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Steps 1 to 7 for all the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Step 1 to 7 for inputs 1 to 3\n",
      "[[0.52353105 0.43351839 0.76328678 0.3449429  0.27203237 0.90431417\n",
      "  0.69152242 0.09339724 0.47987107 0.44888206 0.1571273  0.2378133\n",
      "  0.42837473 0.42222934 0.26950239 0.75302314 0.23953702 0.37005732\n",
      "  0.73004836 0.16261291 0.45862454 0.0296148  0.24272412 0.27160843\n",
      "  0.62116776 0.90029296 0.63565268 0.05190275 0.11249023 0.61936921\n",
      "  0.36791575 0.17169599 0.93257257 0.69532045 0.87245015 0.88797099\n",
      "  0.10272708 0.95414602 0.75713875 0.4566532  0.36617211 0.93953534\n",
      "  0.38317323 0.73428086 0.98699469 0.38373678 0.69603399 0.67249243\n",
      "  0.68768379 0.40616813 0.65463682 0.06796404 0.78165776 0.44594069\n",
      "  0.13907781 0.92005932 0.63079154 0.90278733 0.24098882 0.40180616\n",
      "  0.35490241 0.98548336 0.22918653 0.47448347]\n",
      " [0.70980247 0.26607188 0.94461129 0.39824171 0.24119117 0.94186068\n",
      "  0.54883663 0.22678189 0.9166946  0.26838782 0.39178928 0.20079832\n",
      "  0.24241162 0.50466058 0.13353808 0.50094631 0.56982031 0.99609219\n",
      "  0.56090133 0.33937431 0.78511899 0.62228422 0.08539804 0.03251364\n",
      "  0.68112163 0.63436122 0.74695579 0.80283275 0.74913951 0.86420263\n",
      "  0.78629602 0.29749583 0.8420803  0.5057307  0.40067068 0.04943473\n",
      "  0.40754077 0.99041061 0.62820438 0.74145435 0.89958182 0.56314319\n",
      "  0.57439536 0.56344424 0.23920196 0.45033386 0.84787898 0.43841082\n",
      "  0.76354015 0.43521937 0.70318407 0.0407932  0.07936649 0.11974117\n",
      "  0.6362897  0.12220951 0.54595027 0.74068385 0.4764674  0.79808808\n",
      "  0.07442516 0.43049123 0.35221971 0.43715251]\n",
      " [0.70092775 0.91540783 0.59464577 0.16962222 0.72260282 0.29275112\n",
      "  0.87665442 0.22995589 0.29928027 0.09880669 0.27175636 0.86988155\n",
      "  0.26374285 0.89605643 0.19017923 0.79573684 0.74816273 0.3617837\n",
      "  0.38451691 0.5517812  0.22718592 0.41348336 0.67186465 0.89569834\n",
      "  0.60369153 0.95522023 0.84519514 0.17010481 0.32852405 0.77701583\n",
      "  0.18257714 0.14295673 0.53233035 0.99460068 0.80072527 0.39967001\n",
      "  0.66990414 0.49658655 0.65842684 0.56027913 0.33469861 0.78873312\n",
      "  0.48836309 0.94661111 0.79769419 0.15856827 0.84024571 0.31848655\n",
      "  0.51705249 0.1080445  0.15435828 0.96374117 0.52214043 0.94166586\n",
      "  0.40401538 0.32990045 0.38427613 0.24933372 0.84747447 0.1411553\n",
      "  0.1719641  0.92024271 0.46991132 0.87755014]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 8: Step 1 to 7 for inputs 1 to 3\")\n",
    "#We assume we have 3 results with learned weights (they were not trained in this example)\n",
    "#We assume we are implementing the original Transformer paper. We will have 3 results of 64 dimensions each\n",
    "attention_head1=np.random.random((3, 64))\n",
    "print(attention_head1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: The output of the heads of the attention sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: We assume we have trained the 8 heads of the attention sub-layer\n",
      "shape of one head (3, 64) dimension of 8 heads 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 9: We assume we have trained the 8 heads of the attention sub-layer\")\n",
    "z0h1=np.random.random((3, 64))\n",
    "z1h2=np.random.random((3, 64))\n",
    "z2h3=np.random.random((3, 64))\n",
    "z3h4=np.random.random((3, 64))\n",
    "z4h5=np.random.random((3, 64))\n",
    "z5h6=np.random.random((3, 64))\n",
    "z6h7=np.random.random((3, 64))\n",
    "z7h8=np.random.random((3, 64))\n",
    "print(\"shape of one head\",z0h1.shape,\"dimension of 8 heads\",64*8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Concatenation of the output of the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\n",
      "[[0.01631874 0.19172767 0.75296192 ... 0.30210663 0.51791097 0.26156163]\n",
      " [0.46704406 0.28731821 0.74328609 ... 0.29452945 0.08013049 0.55818484]\n",
      " [0.5968385  0.29230314 0.55923477 ... 0.06511621 0.72178965 0.90935397]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 10: Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\")\n",
    "output_attention=np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n",
    "print(output_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t4nlpacv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
